---
title: "Data 621 - Week4"
author: "Baron Curtin"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    toc: true
    theme: cayman
    highlight: github
    df_print: paged
---

## Cover Page

Data 621 - Week 4 HW

Baron Curtin

CUNY School of Professional Studies


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE)

loadPkg <- function(x) {
  if(!require(x, character.only = T)) install.packages(x, dependencies = T, repos = "http://cran.us.r-project.org")
  require(x, character.only = T)
}

libs <- c("knitr", "magrittr", "data.table", "kableExtra", "caret", "pROC", "missForest", "zoo", "ISLR", "leaps", "fBasics", "reshape2", "tidyverse", "GGally", "gridExtra", "ROCR", "dummies", "pscl")

lapply(libs, loadPkg)
```

```{r}
insTraining <- fread("https://raw.githubusercontent.com/baroncurtin2/data621/master/week4/insurance_training_data.csv") %>%
  as.tibble()
insTest <- fread("https://raw.githubusercontent.com/baroncurtin2/data621/master/week4/insurance-evaluation-data.csv") %>%
  as.tibble()
```



## Introduction
The purpose of this assignment is to generate multiple linear regression and binary logistic regression models based on a training dataset to predict the probability that a person will crash their car and the amount of money the crash costs

## Data Exploration
![](../pics/1.png)

### Non-Visual Inspection

#### Variables
![](./vars.png)

  * Response Variable:
    + TARGET_FLAG: binary indicator of whether the car was involved in a crash
    + TARGET_AMT: cost of the crash
  * Explanatory Variables
```{r}
data_frame(explanatory_variables = names(insTraining)) %>%
  filter(!explanatory_variables %in% c("TARGET_FLAG", "TARGET_AMT", "INDEX")) %>%
  arrange(explanatory_variables)
```

  * Identification Variable:
    + INDEX: will not be used in analysis
    
```{r}
glimpse(insTraining)
```

The glimpse function of dplyr shows that there are `r nrow(insTraining)` observations and `r ncol(insTraining)` variables
  * INCOME, HOME_VAL, OLDCLAIM are interestingly character type but contain monetary values
    + These will have to be converted from char to integer in the data preparation stage
  * MSTATUS, SEX, EDUCATION, URBANiCITY will have to be modified in the data preparation stage as there are inconsitent values

```{r}
insTraining %>%
  sapply(typeof) %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(vartype = 2) %>%
  group_by(vartype) %>%
  summarise(count = n())
```

  * There are 14 fields of the character data type
  * 1 field of the double data type
  * 11 fields of the interger data type


```{r}
summary(insTraining)
```

  * YOJ and AGE have missing values, they will need to be imputed
  * CAR_AGE has a negative value. How is that possible?

#### Basic Stats
```{r}
# function to convert text monetary values to numeric values
convertText2Num <- function(x) {
  x %>%
    # remove symbols and punctuation
    str_replace_all("[\\$[:punct:]]", "") %>%
    # convert to number
    as.numeric() %>%
    # replace NA with 0
    if_else(is.na(.), 0, .)
}

insStats <- insTraining %>%
  mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convertText2Num) %>%
  select_if(is.numeric) %>%
  basicStats(.) %>%
  as_tibble() %>%
  rownames_to_column() %>%
  gather(var, value, -rowname)%>%
  spread(rowname, value) %>%
  rename_all(str_to_lower) %>%
  rename_all(str_trim) %>%
  rename(variables = "var",
         "q1" = `1. quartile`,
         "q3" = `3. quartile`,
         "max"= maximum,
         "min" = minimum,
         "na_vals" = nas, 
         "n" = nobs,
         "sd" = stdev,
         "var" = variance) %>%
  mutate(obs = n - na_vals,
         range = max - min,
         iqr = q3 - q1) %>%
  select(variables, n, na_vals, obs, mean, min, q1, median, q3, max, sd, var, range, iqr, skewness, kurtosis)

insStats
```
  * basicStats further confirms that CAR_AGE and YOJ have missing values
  * HOME_VAL has the highest variance amongst the variables
  * The largest skew value is ~8.7 in the TARGET_AMT variable
  
#### Correlation
```{r}
insTraining %>%
  mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convertText2Num) %>%
  select_if(is.numeric) %>%
  cor(use = "na.or.complete") %>%
  as.data.frame() %>%
  rownames_to_column(var = "predictor") %>%
  as_data_frame() %>%
  select(predictor, TARGET_FLAG, TARGET_AMT) %>%
  filter(!predictor %in% c("INDEX", "TARGET_FLAG", "TARGET_AMT")) %>%
  arrange(desc(TARGET_FLAG)) %>%
  mutate(flag_rank = dense_rank(desc(TARGET_FLAG)),
         amt_rank = dense_rank(desc(TARGET_AMT)),
         rank_equal = flag_rank == amt_rank)
```

  * MVR_PTS has the highest positive correlation with TARGET_FLAG
    + You would typically expect someone with a lot of points on their license to get in more accidents
  * HOME_VAL has the highest negative correlation with TARGET_FLAG
    + I was not expecting HOME_VAL to have the highest negative correlation
  * HOMEKIDS has a positive correlation
    + I would think that parents would be more responsible drivers
  * Rankings were added to show if some predictors are more highly correlated with TARGET_FLAG vs TARGET_AMT
    + BLUEBOOK is more positively correlated with TARGET_AMT than TARGET_FLAG, which is to be expected cause the higher a car's value, the higher the payout


```{r}
ggcorr(insTraining, palette = "RdBu", label = T, geom = "tile", size = 2)
```

  * The correlation matrix above shows that TARGET_FLAG and TARGET_AMT and KIDSDRIV and HOMEKIDS are the two sets of most highly correlated variables
    + This makes sense because when kids are at home, they are likely to drive the parents car because they often can't afford one of their own
    + TARGET_FLAG and TARGET_AMT make sense because if you're in a crash, you are very likely to be paying for the cost of the crash
  * Outside of the two sets of variables outlined above, MVR_PTS and CLM_FREQ are the most highly correlated variables
    + People with points on their record are more likely to file claims more frequently

### Visual Inspection

#### Density Plots
```{r}
vis <- insTraining %>%
  mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convertText2Num) %>%
  select_if(is.numeric) %>%
  melt(id.vars = "INDEX")

ggplot(vis, aes(value)) +
  geom_density(fill = "skyblue") +
  facet_wrap(~ variable, scales = "free")
```

  * AGE is the only normally distributed variable
    + Interesting as this provides some evidence that the data is adequate for inference
  * All of the other variables are multi-modal and assymetric

#### Histograms
```{r}
insTraining %>%
  mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convertText2Num) %>%
  mutate(TARGET_FLAG = as.factor(TARGET_FLAG)) %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 25) +
  facet_wrap(~ key, scales = "free")
```

  * The histograms further reiniforce the conclusions made in the density plots

#### Box Plots  
```{r}
ggplot(vis, aes(x = variable, y = value)) +
  geom_boxplot(show.legend = T) +
  stat_summary(fun.y = mean, color = "red", geom = "point", shape = 18, size = 3) +
  coord_flip()
```

  * HOME_VAL and INCOME appear to have the highest variances
    + This confirms what we saw in basicStats
    
Removing HOME_VAL, INCOME, OLDCLAIM, BLUEBOOK, TARGET_AMT...
```{r}
vis %>%
  filter(!variable %in% c("HOME_VAL", "INCOME", "OLDCLAIM", "BLUEBOOK", "TARGET_AMT")) %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot(show.legend = T) +
  stat_summary(fun.y = mean, color = "red", geom = "point", shape = 18, size = 3) +
  coord_flip()
```

  * The means and medians are pretty close to each other for all of the variables, displaying evidence of very slight skews


## Data Preparation
![](../pics/2.png)

### Transforming Monetary Columns to Numeric

```{r}
set.seed(777)
smp_size <- floor(.7 * nrow(insTraining))
train_ind <- sample(seq_len(nrow(insTraining)), size = smp_size)

datasets <- list(train = insTraining[train_ind, ], test = insTest, test2 = insTraining[-train_ind, ])

# helper function for converting text to number
convertText2Num <- function(x) {
  x %>%
    # remove symbols and punctuation
    str_replace_all("[\\$[:punct:]]", "") %>%
    # convert to number
    as.numeric()
}
```



```{r}
datasets %<>%
  map(function(df) {
    df %<>%
      # convert text to num
      mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convertText2Num)
    
    # return dataframe
    return(df)
  })
```

Converting the currency columns to numeric will be important when modelling

### Transform Inconsitent Observations Within Columns

There was evidence of inconsistent naming across observations. We will transform those variables here. We will first create a list of all the character columns and the values they contain
```{r}
charCols <- datasets$train %>%
  select_if(is.character) %>%
  lapply(function(x) unique(x)) %>%
  print
```

  * Many of the fields have values prefixed with "z_" which can be removed
  * EDUCATION field has an addition "<" that prefixes High School which also can be removed
  * RED_CHAR will be transformed so that the first letter is capitalized
  * URBANICITY will be trimmed to remove extra spaces and renamed to URBANCITY

```{r}
datasets %<>%
  map(function(df) {
    df %<>%
     mutate(EDUCATION = str_replace_all(EDUCATION, "<", ""),
         RED_CAR = str_to_title(RED_CAR),
         URBANICITY = str_replace(URBANICITY, "/ ", "/")) %>%
      mutate_at(names(charCols), str_replace_all, pattern = "z_", replacement = "")
    
    # return df
    return(df)
  })
```

#### Create Binary Dummy Variable Columns for Categorical Variables

First we will take CAR_USE, PARENT1, MSTATUS, SEX, RED_CAR, REVOKED, URBANCITY and convert them to binary variables
```{r}
datasets %<>%
  map(function(df) {
    df %<>%
      # PARENT1 rename to SINGLEPARENT / conversion
      rename(SINGLEPARENT = PARENT1) %>%
      mutate(SINGLEPARENT = if_else(SINGLEPARENT == "Yes", 1, 0)) %>%
      # MSTATUS rename to MARRIED / conversion
      rename(MARRIED = MSTATUS) %>%
      mutate(MARRIED = if_else(MARRIED == "Yes", 1, 0)) %>%
      # SEX rename to MALE / conversion
      rename(MALE = SEX) %>%
      mutate(MALE = if_else(MALE == "M", 1, 0)) %>%
      # CAR_USE rename to COMMERCIALUSE / conversion
      rename(COMMERCIALUSE = CAR_USE) %>%
      mutate(COMMERCIALUSE = if_else(COMMERCIALUSE == "Commercial", 1, 0)) %>%
      # RED_CAR conversion
      mutate(RED_CAR = if_else(RED_CAR == "Yes", 1, 0)) %>%
      # REVOKED conversion
      mutate(REVOKED = if_else(REVOKED == "Yes", 1, 0)) %>%
      # URBANICITY
      mutate(URBANICITY = if_else(str_detect(URBANICITY, "Urban"), 1, 0))
  })
```

  * SINGLEPARENT will have value of 1 when TRUE, else 0
  * MARRIED will have value of 1 when TRUE, else 0
  * RED_CAR will have value of 1 when TRUE, else 0
  * MALE will have value of 1 when TRUE, else 0
  * COMMERCIALUSE will have value of 1 when TRUE, else 0
  * REVOKED will have value of 1 when TRUE, else 0
  * URBANICITY will have value of 1 when TRUE, else 0


The rest of the variables will be dummified using the dummies package
```{r}
datasets %<>%
  map(function(df) {
    # create dummy data frame
    df <- df %>%
      as.data.frame() %>%
      dummy.data.frame(names = c("EDUCATION", "JOB", "CAR_TYPE"))
    
    # return df
    return(df)
  })
```

  * Converting all of the character fields to to binary columns will make missForest's job easier as well as provide a better regression model
  * The remaining variables are EDUCATION, JOB, CAR_TYPE

The fields with spaces will be renamed to include _  
```{r}
replace_space <- function(x) str_replace(x, " ", "_")

datasets %<>%
  map(function(df) {
    # rename all with spaces
    df %<>%
      rename_all(replace_space)
    
    # return df
    return(df)
  })
```



### Missing Value Imputation
We can use the package missForest to impute values for the NAs
```{r}
forests <- datasets %>%
  map(function(df) {
    df %<>%
      as.data.frame() %>%
      select_if(is.numeric) %>%
      select(-INDEX) %>%
      missForest()
    
    # return df
    return(df)
  })
```

```{r}
imputed <- forests %>%
  map(function(x) {
    return(x$ximp)
  })
```


## Build Models
![](../pics/3.png)

### Leaps Subsetting
We can use the leaps package to subset the explanatory variables to find the best model. All of the character variables have already been converted to factors and subsequently numerical values and all of the missing values have been imputed using missForest

```{r}
mlrs <- regsubsets(TARGET_AMT ~ . -TARGET_FLAG, data = imputed$train, method = "exhaustive", nvmax = NULL, nbest = 1)
mlrs.summary <- summary(mlrs)
print(mlrs.summary)
```

We can determine the best fits by ploting the number of variables in our subsets and using the cp value in combinationation with the adjusted R^2 value
```{r}
# determine best subset
plot(mlrs.summary$cp, xlab = "Number of Variables", ylab = "Cp")
points(which.min(mlrs.summary$cp), mlrs.summary$cp[which.min(mlrs.summary$cp)], pch = 20, col = "red")

# cp plot
# par(mfrow=c(1,2))
plot(mlrs, scale = "Cp", main = "Cp")

# r^2 splot
plot(mlrs, scale = "adjr2", main = "Adjusted R^2")
```

  * Based on the diagnostic plot, 20 variables appear to fit the data best
  


### Multiple Linear Regression

#### Model 1: All Variables
We will first create a linear model with all of the predictors. It will most likely overfit the data but provides a good reference point. We also know from leaps, that the best model will use 16 of the predictors

```{r}
m1 <- lm(TARGET_AMT ~ . -TARGET_FLAG, data = imputed$train)
summary(m1)
```

  * There are 16 significant variables in Model 1, 6 of which are highly significant at a 5% significance level
  * The adjusted R^2 is .06737
  * The 3 variables CAR_TYPEVan, JOBStudent, and EDUCATIONPhD are NA
  * Interesting Positive Predictors
    + SINGLEPARENT has a positive impact on TARGET_AMT. One would expect single parents to be more responsible because they are the only care providers for their children
    + Home makers have the highest positive impact on TARGET_AMT
    + COMMERCIALUSE vehicles have a higher positive impact
    + Urban areas as expected are more prone to crashes due to population
  * Interesting Negative Predictors
    + Bachelor degrees have a higher negative impact on TARGET_AMT. This may be due to a lack of people pursuing education past a Bachelors
    + Doctors have one of the highest negative impacts on TARGET_AMT. They see victims of car crashes
    + RED_CAR has a negative impact. It is thought that red cars make people speed more
    
    
#### Model 2: 20 Variables Chosen by leaps
```{r}
m2 <- lm(TARGET_AMT ~ KIDSDRIV + INCOME + SINGLEPARENT + MARRIED + MALE + EDUCATIONBachelors + JOBDoctor + JOBManager + TRAVTIME + COMMERCIALUSE + BLUEBOOK + TIF + CAR_TYPEMinivan + CAR_TYPESports_Car + CAR_TYPESUV + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = imputed$train)
summary(m2)
```

  * The adjusted R^2 of Model 2 is .06792, still very small but slightly improved over Model 1
  * Of the 20 variables, 16 are significant, while 10 are highly significant at the 5% level
  * Interesting Positive Predictors
    + COMMERCIALUSE is the highest positive influencer
    + SINGLEPARENT is still a high positive influencer
  * Interesting Negative Predictors
    + JOBManager has overtaken JOBDoctor as the highest negative influencer
    
#### Model 3: 1 + Log Transform 20 Variables from Leaps
```{r}
m3 <- imputed$train %>%
  mutate_all(~ log(1 + .x)) %>%
  lm(TARGET_AMT ~ KIDSDRIV + INCOME + SINGLEPARENT + MARRIED + MALE + EDUCATIONBachelors + JOBDoctor + JOBManager + TRAVTIME + COMMERCIALUSE + BLUEBOOK + TIF + CAR_TYPEMinivan + CAR_TYPESports_Car + CAR_TYPESUV + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = .)

summary(m3)
```

  * R^2 has greatly improved to .2159
  * Interesting positive predictors
    + URBANICITY is now the highest positive influencer
    + COMMERCIALUSE still has a strong positive impact on TARGET_AMT
  * Interesting negative predictors
    + JOBManager is still the highest negative influencer
    
### Binary Logistic Regression

#### Model 4: All Variables
```{r}
m4 <- glm(TARGET_FLAG ~ . -TARGET_AMT, data = imputed$train, family = "binomial")
summary(m4)
pR2(m4)
```

  * Using the pscl package, we can get a pseudo-R^2 value of ~.2304
  * There are 19 significant variables in Model 4, 16 of which are highly significant at a 5% significance level
  * The 3 variables CAR_TYPEVan, JOBStudent, and EDUCATIONPhD are NA
  * Interesting Positive Predictors
    + SINGLEPARENT has a positive impact on TARGET_AMT. One would expect single parents to be more responsible because they are the only care providers for their children
    + REVOKED has the highest positive impact on TARGET_FLAG
    + Urban areas as expected are more prone to crashes probably due to population
    + High school has a positive impact, as opposed to the negative impact on TARGET_AMT
  * Interesting Negative Predictors
    + Bachelor degrees have a higher negative impact on TARGET_AMT. This may be due to a lack of people pursuing education past a Bachelors
    + Managers have one of the highest negative impacts on TARGET_AMT. They see victims of car crashes
    + RED_CAR has a negative impact. It is thought that red cars make people speed more
    
#### Model 5: 20 Variables Chosen by leaps (Untransformed)
```{r}
m5 <- glm(TARGET_FLAG ~ KIDSDRIV + INCOME + SINGLEPARENT + MARRIED + MALE + EDUCATIONBachelors + JOBDoctor + JOBManager + TRAVTIME + COMMERCIALUSE + BLUEBOOK + TIF + CAR_TYPEMinivan + CAR_TYPESports_Car + CAR_TYPESUV + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = imputed$train, family = "binomial")
summary(m5)
pR2(m5)
```

  * Using the pscl package, we can get a pseudo-R^2 value of ~.2216
  * Interesting Positive Predictors
    + COMMERCIALUSE now has the highest predictor
  * Interesting Negative Predictors
    + Managers are still the lowest negative predictor. Could be due to age
    + Minivans have the lowest impact on crashes amongst the car types
  * MALE is not a significant predictor but it has a highest impact
  
#### Model 6: 1 + Log Transform 20 Variables Chosen by leaps
```{r}
d <- imputed$train %>%
  mutate_all(~ log(1 + .x))

m6 <- glm(TARGET_FLAG ~ KIDSDRIV + INCOME + SINGLEPARENT + MARRIED + MALE + EDUCATIONBachelors + JOBDoctor + JOBManager + TRAVTIME + COMMERCIALUSE + BLUEBOOK + TIF + CAR_TYPEMinivan + CAR_TYPESports_Car + CAR_TYPESUV + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = d, family = "binomial")
summary(m6)
pR2(m6)
```

  * Using the pscl package, we can get a pseudo-R^2 value of ~.1987
  * Interesting Positive Predictors
    + URBANICITY now has the highest predictor
    + SUV has a positie predictor
  * Interesting Negative Predictors
    + Managers are still the lowest negative predictor. Could be due to age
    + Minivans have the lowest impact on crashes amongst the car types
  * MALE is not a significant predictor but it has a highest impact

## Select Models
![](../pics/4.png)

We will use Model 3 and Model 5 based on their R^2 values and the fact that all or most of the variables are significant at the 5% level

### Model 3
```{r}
par(mfrow=c(2,2))

plot(m3)
hist(m3$residuals)
qqnorm(m3$residuals)
qqline(m3$residuals)
```

  * The histogram of the residuals do not show a normal distribution
  * The qqplot shows a fairly linear relationship with the tails of the plot venturing away from the line
  * The residual plot does not display contant variance
  
#### Test Model
```{r}
m3_results <- predict(m3, newdata = datasets$test2)
datasets$test <- bind_cols(datasets$test2, data_frame(m3results = m3_results)) %>%
  mutate(m3results = if_else(m3results < 0, 0, m3results),
         m3amt_match = TARGET_AMT == m3results)
datasets$test
```

### Model 5
```{r}
par(mfrow=c(2,2))

plot(m5)
hist(m5$residuals)
qqnorm(m5$residuals)
qqline(m5$residuals)
```

  * The histogram of the residuals do not show a normal distribution
  * The qqplot shows a fairly linear relationship that ventures wildly away from linear towards the upper tail
  * The residual plot shows no evidence of a constant variance
  
#### Test Model
```{r}
m5_results <- predict(m5, newdata = datasets$test2, type = "response")
datasets$test <- bind_cols(datasets$test2, data_frame(m5results = m5_results)) %>%
  mutate(m5results = if_else(m5_results > mean(TARGET_FLAG), 1, 0),
         m5flag_match = m5results == TARGET_FLAG)
datasets$test

mean(datasets$test$m5flag_match, na.rm = T)
```

  * The linear model correctly predicted at a 72% rate

#### Performance
```{r}
cm <- confusionMatrix(as.factor(datasets$test$m5results), as.factor(datasets$test$TARGET_FLAG), positive = "1", mode = "everything") %>%
  print

curveRoc <- roc(datasets$test$m5results, datasets$test$TARGET_FLAG)
plot(curveRoc, legacy.axes = T, main = "pROC")
```

  * The accuracy is only ~73%
  * Positive prediction was very poor at below 50%
  * Negative prediction was decent sitting at ~89%
  * Sensitivity is ~75%
  * Specificity is ~72%
  * The F1 is ~60%
  * The AUC is `r curveRoc$auc`
  * Over all, I'd say the model is pretty underwhelming in prediction power

## Code Appendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```